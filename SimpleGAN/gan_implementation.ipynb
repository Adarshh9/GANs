{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTPiw9eIDQ8p"
      },
      "outputs": [],
      "source": [
        "from keras.datasets.cifar10 import load_data\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "(xtrain ,ytrain) ,(xtest ,ytest) = load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "  plt.subplot(10 ,10 ,1+i)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(xtrain[i])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kbtSK6K_FeVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(xtrain.shape)"
      ],
      "metadata": {
        "id": "jkNhUxxWFoSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining Descriminator model\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense ,Conv2D ,Flatten ,Dropout ,LeakyReLU\n",
        "from keras.utils import plot_model\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "SFsV2pkIGF5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_descriminator(in_shape=(32,32,3)):\n",
        "\n",
        "  model = Sequential([\n",
        "\n",
        "      Conv2D(filters=64 ,kernel_size=(3,3) ,padding='same' ,input_shape=in_shape) ,\n",
        "      LeakyReLU(alpha=0.2) ,\n",
        "\n",
        "      Conv2D(filters=128 ,kernel_size=(3,3) ,padding='same' ,strides=(2,2)) ,\n",
        "      LeakyReLU(alpha=0.2) ,\n",
        "\n",
        "      Conv2D(filters=128 ,kernel_size=(3,3) ,padding='same' ,strides=(2,2)) ,\n",
        "      LeakyReLU(alpha=0.2) ,\n",
        "\n",
        "      Conv2D(filters=256 ,kernel_size=(3,3) ,padding='same' ,strides=(2,2)) ,\n",
        "      LeakyReLU(alpha=0.2) ,\n",
        "\n",
        "      Flatten() ,\n",
        "      Dropout(0.4) ,\n",
        "      Dense(units=1 ,activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  optimizer = Adam(learning_rate=0.0002 ,beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy' ,optimizer=optimizer ,metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "pvBvJ9klHmjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = define_descriminator()\n",
        "# model.summary()"
      ],
      "metadata": {
        "id": "lYZgYBGIJ7cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_model(model=model ,to_file='descriminator.png' ,show_shapes=True)"
      ],
      "metadata": {
        "id": "8Ou38ozhKP-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_real_samples():\n",
        "\n",
        "  (xtrain,_) ,(_,_) = load_data()\n",
        "  X = xtrain.astype('float32')\n",
        "  X = (X - 127.5) / 127.5\n",
        "  return X"
      ],
      "metadata": {
        "id": "H6bq_QhsKrmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X = load_real_samples()\n",
        "# X.shape"
      ],
      "metadata": {
        "id": "0NDrIwhBMjic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X[0]"
      ],
      "metadata": {
        "id": "SZHZ_UUbM5yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_real_samples(dataset, n_samples):\n",
        "    half_batch = int(n_samples / 2)\n",
        "    ix = np.random.randint(0, dataset.shape[0], half_batch)\n",
        "    X = dataset[ix]\n",
        "    y = np.ones((half_batch, 1))\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "dxPhtI2FNEnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X ,y = generate_real_samples(X ,64)\n",
        "# X.shape"
      ],
      "metadata": {
        "id": "v8Kl3XxxWXqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_fake_samples(n_samples):\n",
        "    size = int(32 * 32 * 3 * n_samples)\n",
        "    X = np.random.rand(size)\n",
        "    X = -1 + X * 2\n",
        "    X = X.reshape((n_samples, 32, 32, 3))\n",
        "    y = np.zeros((n_samples, 1))\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "P3Tas_Z1WjoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X ,y = generate_fake_samples(64)\n",
        "X.shape"
      ],
      "metadata": {
        "id": "hD5KpNUkXfR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_descriminator(model ,dataset ,n_iter=20 ,n_batch=128):\n",
        "  half_batch = n_batch / 2\n",
        "  for i in range(n_iter):\n",
        "    x_real ,y_real = generate_real_samples(dataset ,half_batch)\n",
        "    _ ,train_acc = model.train_on_batch(x_real ,y_real)\n",
        "\n",
        "    x_fake ,y_fake = generate_fake_samples(int(half_batch))\n",
        "    _ ,test_acc = model.train_on_batch(x_fake ,y_fake)\n",
        "\n",
        "    print(f'real : {train_acc*100}%  fake : {test_acc*100}%')"
      ],
      "metadata": {
        "id": "N3ezUI99YAg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = define_descriminator()\n",
        "dataset = load_real_samples()\n",
        "# dataset\n",
        "train_descriminator(model ,dataset)"
      ],
      "metadata": {
        "id": "P8BAD5cZal84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generator"
      ],
      "metadata": {
        "id": "46V1AttviHGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense ,Reshape ,Conv2D ,Conv2DTranspose ,LeakyReLU\n",
        "from keras.utils import plot_model"
      ],
      "metadata": {
        "id": "BXvxl74rnwT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining Generator model\n",
        "def define_generator(latent_dim):\n",
        "  n_nodes = 256 *4 *4\n",
        "  model = Sequential([\n",
        "      #foundation for 4x4 image\n",
        "      Dense(units=n_nodes ,input_dim=latent_dim) ,\n",
        "      LeakyReLU(alpha=0.2) ,\n",
        "      Reshape((4 ,4 ,256)) ,\n",
        "\n",
        "      #upsample to 8x8\n",
        "      Conv2DTranspose(filters=128 ,kernel_size=(4,4) ,strides=(2,2) ,padding='same') ,\n",
        "      LeakyReLU(alpha=0.2) ,\n",
        "\n",
        "      #upsample to 16x16\n",
        "      Conv2DTranspose(filters=128 ,kernel_size=(4,4) ,strides=(2,2) ,padding='same') ,\n",
        "      LeakyReLU(alpha=0.2) ,\n",
        "\n",
        "      #upsample to 32x32\n",
        "      Conv2DTranspose(filters=128 ,kernel_size=(4,4) ,strides=(2,2) ,padding='same') ,\n",
        "      LeakyReLU(alpha=0.2) ,\n",
        "\n",
        "      Conv2D(filters=3 ,kernel_size=(3,3) ,activation='relu' ,padding='same')\n",
        "  ])\n",
        "  return model"
      ],
      "metadata": {
        "id": "M-qi-8d9igeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 100\n",
        "gmodel = define_generator(latent_dim)\n",
        "gmodel.summary()"
      ],
      "metadata": {
        "id": "iFUY4CwbnTQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model=gmodel ,to_file='generator.png' ,show_shapes=True)"
      ],
      "metadata": {
        "id": "HhiJpDCTngUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_latent_points(latent_dim ,n_samples):\n",
        "  x_input = np.random.randn(latent_dim * n_samples)\n",
        "  x_input = x_input.reshape(n_samples ,latent_dim)\n",
        "  return x_input"
      ],
      "metadata": {
        "id": "4H9ZlOYcnnWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_input = generate_latent_points(latent_dim=100 ,n_samples=64)\n",
        "x_input.shape"
      ],
      "metadata": {
        "id": "pyoj9xqJpLDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_fake_samples(gmodel ,latent_dim ,n_samples):\n",
        "  x_input = generate_latent_points(latent_dim ,n_samples)\n",
        "  X = gmodel.predict(x_input)\n",
        "  y = np.zeros((n_samples ,1))\n",
        "  return X ,y"
      ],
      "metadata": {
        "id": "A908EIcipUo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 100\n",
        "gmodel = define_generator(latent_dim)\n",
        "\n",
        "n_samples = 25\n",
        "\n",
        "X ,_ = generate_fake_samples(gmodel ,latent_dim ,n_samples)\n",
        "X = (X + 1) / 2.0\n",
        "\n",
        "for i in range(n_samples):\n",
        "  plt.subplot(5 ,5 ,i+1)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(X[i])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4pXefAcLuS8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_gan(g_model ,d_model):\n",
        "  d_model.trainable = False\n",
        "\n",
        "  model = Sequential([\n",
        "      g_model ,\n",
        "      d_model\n",
        "  ])\n",
        "  opt = Adam(learning_rate=0.0002 ,beta_1=0.5)\n",
        "  model.compile(optimizer=opt ,loss='binary_crossentropy' ,metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "iUEgCknwvG4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 100\n",
        "\n",
        "d_model = define_descriminator()\n",
        "g_model = define_generator(latent_dim)\n",
        "\n",
        "gan_model = define_gan(g_model ,d_model)\n",
        "\n",
        "gan_model.summary()\n",
        "\n",
        "plot_model(gan_model ,to_file='gan.png' ,show_shapes=True ,show_layer_names=True)"
      ],
      "metadata": {
        "id": "lX1HqL2Dymsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the Generator and Descriminator\n",
        "def train_gan(g_model ,d_model ,gan_model ,dataset ,latent_dim ,n_epochs=200 ,n_batch=128):\n",
        "  batch_per_epoch = int(dataset.shape[0] / n_batch)\n",
        "  half_batch = int(n_batch / 2)\n",
        "\n",
        "  for i in range(n_epochs):\n",
        "    for j in range(batch_per_epoch):\n",
        "      X_real , y_real = generate_real_samples(dataset ,half_batch)\n",
        "      d_loss1 , _ = d_model.train_on_batch(X_real ,y_real)\n",
        "\n",
        "      X_fake , y_fake = generate_fake_samples(g_model ,latent_dim ,half_batch)\n",
        "      d_loss2 , _ = d_model.train_on_batch(X_fake ,y_fake)\n",
        "\n",
        "      X_gan = generate_latent_points(latent_dim ,n_batch)\n",
        "      y_gan = np.ones((n_batch ,1))\n",
        "\n",
        "      g_loss = gan_model.train_on_batch(X_gan ,y_gan)\n",
        "\n",
        "      print(f'>{i+1}, {j+1}/{batch_per_epoch}, d1={d_loss1} ,d2={d_loss2} ,g={g_loss}')\n",
        "\n",
        "    if (i+1) % 10 == 0:\n",
        "      summarize_performance(i ,g_model ,d_model ,dataset ,latent_dim)"
      ],
      "metadata": {
        "id": "_mpzzF3KzowJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Descriminator ,plot generated images ,save generator model\n",
        "def summarize_performance(i ,g_model ,d_model ,dataset ,latent_dim):\n",
        "  X_real , y_real = generate_real_samples(dataset ,n_samples)\n",
        "  _ , acc_real = d_model.evaluate(X_real ,y_real ,verbose=0)\n",
        "\n",
        "  X_fake , y_fake = generate_fake_samples(g_model ,latent_dim ,n_samples)\n",
        "  _ , acc_fake = d_model.evaluate(X_fake ,y_fake ,verbose=0)\n",
        "\n",
        "  print(f'>Accuracy -->  real:{acc_real*100}% fake:{acc_fake*100}%')\n",
        "\n",
        "  save_plot(X_fake ,epoch=i)\n",
        "\n",
        "  filename = f'generator_model_{i+1}.h5'\n",
        "  g_model.save(filename)"
      ],
      "metadata": {
        "id": "ugCW8ixK3mVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_plot(examples ,epoch ,n=5):\n",
        "  examples = (examples + 1) / 2.0\n",
        "\n",
        "  for i in range(n*n):\n",
        "    plt.subplot(n ,n ,i+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(examples[i])\n",
        "  filename = f'generator_model_{epoch+1}.png'\n",
        "  plt.savefig(filename)\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "1acIDT8d5nN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "taOTBqZS9rfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd GAN"
      ],
      "metadata": {
        "id": "ZRrknRyH9tYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "mDMEH_il98_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gan(g_model ,d_model ,gan_model ,dataset ,latent_dim ,n_epochs=200 ,n_batch=128)"
      ],
      "metadata": {
        "id": "smVVqxIb6Wx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EH5DkiZC6pzT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}