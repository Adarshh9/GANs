{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_R4u4ggOgFJ"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input ,Dense ,Reshape ,Flatten\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "from keras.models import Sequential ,Model\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_rows = 28\n",
        "img_cols = 28\n",
        "channels = 1\n",
        "img_shape = (img_rows ,img_cols ,channels)"
      ],
      "metadata": {
        "id": "dM7dF7BVPnd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def built_generator():\n",
        "    noise_shape = (100 ,0)\n",
        "\n",
        "    model = Sequential([\n",
        "        Dense(256 ,input_shape= noise_shape) ,\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        BatchNormalization(momentum=0.8),\n",
        "\n",
        "        Dense(512 ,input_shape= noise_shape) ,\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        BatchNormalization(momentum=0.8),\n",
        "\n",
        "        Dense(1024 ,input_shape= noise_shape) ,\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        BatchNormalization(momentum=0.8),\n",
        "\n",
        "        Dense(np.prod(img_shape) ,activation='tanh'),\n",
        "        Reshape(img_shape)\n",
        "    ])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    noise = Input(shape = noise_shape)\n",
        "    img = model(noise)\n",
        "\n",
        "    return Model(noise ,img)"
      ],
      "metadata": {
        "id": "ROZ2DOsrQD7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator():\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=img_shape),\n",
        "        Dense(512),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "\n",
        "        Dense(256),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "\n",
        "        Dense(1 ,activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    img = Input(shape=img_shape)\n",
        "    validity = model(img)\n",
        "\n",
        "    return Model(img ,validity)"
      ],
      "metadata": {
        "id": "Asb7ktjmdUEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##OR\n",
        "def build_generator():\n",
        "\n",
        "    noise_shape = (100,) #1D array of size 100 (latent vector / noise)\n",
        "\n",
        "#Define your generator network\n",
        "#Here we are only using Dense layers. But network can be complicated based\n",
        "#on the application. For example, you can use VGG for super res. GAN.\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(256, input_shape=noise_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
        "    model.add(Reshape(img_shape))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    noise = Input(shape=noise_shape)\n",
        "    img = model(noise)    #Generated image\n",
        "\n",
        "    return Model(noise, img)"
      ],
      "metadata": {
        "id": "u3cg12YjoUkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs ,batch_size=128 ,save_interval=50):\n",
        "    (X_train ,_) ,(_, _) = mnist.load_data()\n",
        "\n",
        "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "    X_train = np.expand_dims(X_train ,axis=3)\n",
        "\n",
        "    half_batch = int(batch_size / 2)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        idx = np.random.randint(0 ,X_train.shape[0] ,half_batch)\n",
        "        imgs = X_train[idx]\n",
        "\n",
        "        noise = np.random.normal(0 ,1 ,(half_batch ,100))\n",
        "\n",
        "        gen_imgs = generator.predict(noise)\n",
        "\n",
        "        d_loss_real = discriminator.train_on_batch(imgs ,np.ones((half_batch ,1)))\n",
        "        d_loss_fake = discriminator.train_on_batch(gen_imgs ,np.zeros((half_batch ,1)))\n",
        "\n",
        "        d_loss = 0.5 * np.add(d_loss_real ,d_loss_fake)\n",
        "\n",
        "        noise = np.random.normal(0 ,1 ,(batch_size ,100))\n",
        "        valid_y = np.array([1]* batch_size)\n",
        "\n",
        "        g_loss = combined.train_on_batch(noise ,valid_y)\n",
        "\n",
        "        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "        # If at save interval => save generated image samples\n",
        "        if epoch % save_interval == 0:\n",
        "            save_imgs(epoch)"
      ],
      "metadata": {
        "id": "whsm6yBSeeO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_imgs(epoch):\n",
        "    r, c = 5, 5\n",
        "    noise = np.random.normal(0, 1, (r * c, 100))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    # Rescale images 0 - 1\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    fig.savefig(\"/content/images/mnist_%d.png\" % epoch)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "HYdr99RMmYMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer = Adam(0.0002 ,0.5)\n",
        "optimizer = keras.optimizers.legacy.Adam(0.0002 ,0.5)\n",
        "\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss= 'binary_crossentropy' ,optimizer=optimizer ,metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "bER-Adwomh3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = build_generator()\n",
        "generator.compile(loss= 'binary_crossentropy' ,optimizer=optimizer)"
      ],
      "metadata": {
        "id": "kb44DlRJn2wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = Input(shape=(100 ,))\n",
        "img = generator(z)"
      ],
      "metadata": {
        "id": "c2WRaYIloEIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator.trainable = False"
      ],
      "metadata": {
        "id": "WUZXhQKVpOUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid = discriminator(img)"
      ],
      "metadata": {
        "id": "eAdSV4VApaL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined = Model(z ,valid)\n",
        "combined.compile(loss= 'binary_crossentropy' ,optimizer=optimizer)\n",
        "\n",
        "train(epochs=10000 ,batch_size=32 ,save_interval=1000)"
      ],
      "metadata": {
        "id": "BDX9rcYMpvSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yNwmfPbJqEiN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}