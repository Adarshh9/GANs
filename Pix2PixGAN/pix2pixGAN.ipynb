{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8E4XTad9pBX"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "pix2pix GAN model\n",
        "\n",
        "Based on the code by Jason Brownlee from his blogs on https://machinelearningmastery.com/\n",
        "I seriously urge everyone to foloow his blogs and get enlightened.\n",
        "I am adapting his code to various applications but original credit goes to Jason.\n",
        "\n",
        "\n",
        "    Original paper: https://arxiv.org/pdf/1611.07004.pdf\n",
        "    Github for original paper: https://phillipi.github.io/pix2pix/\n",
        "\n",
        "\n",
        "Generator:\n",
        "The encoder-decoder architecture consists of:\n",
        "encoder:\n",
        "C64-C128-C256-C512-C512-C512-C512-C512\n",
        "decoder:\n",
        "CD512-CD512-CD512-C512-C256-C128-C64\n",
        "\n",
        "\n",
        "Discriminator\n",
        "C64-C128-C256-C512\n",
        "After the last layer, a convolution is applied to map to\n",
        "a 1-dimensional output, followed by a Sigmoid function.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy.random import randint\n",
        "from keras.optimizers import Adam\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D ,Input\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.utils import plot_model"
      ],
      "metadata": {
        "id": "7EM5m6-7Bsuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "#Define generator, discriminator, gan and other helper functions\n",
        "#We will use functional way of defining model and not sequential\n",
        "#as we have multiple inputs; both images and corresponding labels.\n",
        "########################################################################\n",
        "\n",
        "#Since pix2pix is a conditional GAN, it takes 2 inputs - image and corresponding label\n",
        "#For pix2pix the label will be another image.\n",
        "\n",
        "# define the standalone discriminator model\n",
        "#Given an input image, the Discriminator outputs the likelihood of the image being real.\n",
        "    #Binary classification - true or false (1 or 0). So using sigmoid activation.\n",
        "#Think of discriminator as a binary classifier that is classifying images as real/fake.\n",
        "\n",
        "# From the paper C64-C128-C256-C512\n",
        "#After the last layer, conv to 1-dimensional output, followed by a Sigmoid function.\n"
      ],
      "metadata": {
        "id": "qg8CCo5pBz2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_discriminator(image_shape):\n",
        "\n",
        "    # weight initialization\n",
        "    init = RandomNormal(stddev=0.02) #As described in the original paper\n",
        "\n",
        "    # source image input\n",
        "    in_src_image = Input(shape=image_shape)  #Image we want to convert to another image\n",
        "    # target image input\n",
        "    in_target_image = Input(shape=image_shape)  #Image we want to generate after training.\n",
        "\n",
        "    # concatenate images, channel-wise\n",
        "    merged = Concatenate()([in_src_image, in_target_image])\n",
        "\n",
        "    # C64: 4x4 kernel Stride 2x2\n",
        "    d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
        "    d = LeakyReLU(alpha=0.2)(d)\n",
        "    # C128: 4x4 kernel Stride 2x2\n",
        "    d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "    d = BatchNormalization()(d)\n",
        "    d = LeakyReLU(alpha=0.2)(d)\n",
        "    # C256: 4x4 kernel Stride 2x2\n",
        "    d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "    d = BatchNormalization()(d)\n",
        "    d = LeakyReLU(alpha=0.2)(d)\n",
        "    # C512: 4x4 kernel Stride 2x2\n",
        "      # Not in the original paper. Comment this block if you want.\n",
        "    d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "    d = BatchNormalization()(d)\n",
        "    d = LeakyReLU(alpha=0.2)(d)\n",
        "    # second last output layer : 4x4 kernel but Stride 1x1\n",
        "    d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
        "    d = BatchNormalization()(d)\n",
        "    d = LeakyReLU(alpha=0.2)(d)\n",
        "    # patch output\n",
        "    d = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
        "    patch_out = Activation('sigmoid')(d)\n",
        "    # define model\n",
        "    model = Model([in_src_image, in_target_image], patch_out)\n",
        "    # compile model\n",
        "      #The model is trained with a batch size of one image and Adam opt.\n",
        "      #with a small learning rate and 0.5 beta.\n",
        "      #The loss for the discriminator is weighted by 50% for each model update.\n",
        "\n",
        "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
        "    return model"
      ],
      "metadata": {
        "id": "J1uUYm5gCLo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################\n",
        "#Now define the generator - in our case we will define a U-net\n",
        "# define an encoder block to be used in generator\n",
        "\n",
        "def define_encoder_block(layer_in, n_filters, batchnorm=True):\n",
        "    # weight initialization\n",
        "    init = RandomNormal(stddev=0.02)\n",
        "    # add downsampling layer\n",
        "    g = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
        "    # conditionally add batch normalization\n",
        "    if batchnorm:\n",
        "      g = BatchNormalization()(g, training=True)\n",
        "    # leaky relu activation\n",
        "    g = LeakyReLU(alpha=0.2)(g)\n",
        "    return g\n",
        "\n",
        "# define a decoder block to be used in generator\n",
        "def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n",
        "    # weight initialization\n",
        "    init = RandomNormal(stddev=0.02)\n",
        "    # add upsampling layer\n",
        "    g = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
        "    # add batch normalization\n",
        "    g = BatchNormalization()(g, training=True)\n",
        "    # conditionally add dropout\n",
        "    if dropout:\n",
        "      g = Dropout(0.5)(g, training=True)\n",
        "    # merge with skip connection\n",
        "    g = Concatenate()([g, skip_in])\n",
        "    # relu activation\n",
        "    g = Activation('relu')(g)\n",
        "    return g"
      ],
      "metadata": {
        "id": "zsPMoBBpCZG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the standalone generator model - U-net\n",
        "def define_generator(image_shape=(256,256,3)):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# image input\n",
        "\tin_image = Input(shape=image_shape)\n",
        "\t# encoder model: C64-C128-C256-C512-C512-C512-C512-C512\n",
        "\te1 = define_encoder_block(in_image, 64, batchnorm=False)\n",
        "\te2 = define_encoder_block(e1, 128)\n",
        "\te3 = define_encoder_block(e2, 256)\n",
        "\te4 = define_encoder_block(e3, 512)\n",
        "\te5 = define_encoder_block(e4, 512)\n",
        "\te6 = define_encoder_block(e5, 512)\n",
        "\te7 = define_encoder_block(e6, 512)\n",
        "\t# bottleneck, no batch norm and relu\n",
        "\tb = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n",
        "\tb = Activation('relu')(b)\n",
        "\t# decoder model: CD512-CD512-CD512-C512-C256-C128-C64\n",
        "\td1 = decoder_block(b, e7, 512)\n",
        "\td2 = decoder_block(d1, e6, 512)\n",
        "\td3 = decoder_block(d2, e5, 512)\n",
        "\td4 = decoder_block(d3, e4, 512, dropout=False)\n",
        "\td5 = decoder_block(d4, e3, 256, dropout=False)\n",
        "\td6 = decoder_block(d5, e2, 128, dropout=False)\n",
        "\td7 = decoder_block(d6, e1, 64, dropout=False)\n",
        "\t# output\n",
        "\tg = Conv2DTranspose(image_shape[2], (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7) #Modified\n",
        "\tout_image = Activation('tanh')(g)  #Generates images in the range -1 to 1. So change inputs also to -1 to 1\n",
        "\t# define model\n",
        "\tmodel = Model(in_image, out_image)\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "IuFm9chCCos8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the combined generator and discriminator model, for updating the generator\n",
        "def define_gan(g_model, d_model, image_shape):\n",
        "\t# make weights in the discriminator not trainable\n",
        "\tfor layer in d_model.layers:\n",
        "\t\tif not isinstance(layer, BatchNormalization):\n",
        "\t\t\tlayer.trainable = False       #Descriminator layers set to untrainable in the combined GAN but\n",
        "                                                #standalone descriminator will be trainable.\n",
        "\n",
        "\t# define the source image\n",
        "\tin_src = Input(shape=image_shape)\n",
        "\t# suppy the image as input to the generator\n",
        "\tgen_out = g_model(in_src)\n",
        "\t# supply the input image and generated image as inputs to the discriminator\n",
        "\tdis_out = d_model([in_src, gen_out])\n",
        "\t# src image as input, generated image and disc. output as outputs\n",
        "\tmodel = Model(in_src, [dis_out, gen_out])\n",
        "\t# compile model\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\n",
        "    #Total loss is the weighted sum of adversarial loss (BCE) and L1 loss (MAE)\n",
        "    #Authors suggested weighting BCE vs L1 as 1:100.\n",
        "\tmodel.compile(loss=['binary_crossentropy', 'mae'],\n",
        "               optimizer=opt, loss_weights=[1,100])\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "jyfqDryZGkak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select a batch of random samples, returns images and target\n",
        "def generate_real_samples(dataset, n_samples, patch_shape):\n",
        "\t# unpack dataset\n",
        "\ttrainA, trainB = dataset\n",
        "\t# choose random instances\n",
        "\tix = randint(0, trainA.shape[0], n_samples)\n",
        "\t# retrieve selected images\n",
        "\tX1, X2 = trainA[ix], trainB[ix]\n",
        "\t# generate 'real' class labels (1)\n",
        "\ty = ones((n_samples, patch_shape, patch_shape, 1))\n",
        "\treturn [X1, X2], y\n",
        "\n",
        "# generate a batch of images, returns images and targets\n",
        "def generate_fake_samples(g_model, samples, patch_shape):\n",
        "\t# generate fake instance\n",
        "\tX = g_model.predict(samples)\n",
        "\t# create 'fake' class labels (0)\n",
        "\ty = zeros((len(X), patch_shape, patch_shape, 1))\n",
        "\treturn X, y"
      ],
      "metadata": {
        "id": "FSP4WRekHomZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate samples and save as a plot and save the model\n",
        "#GAN models do not converge, we just want to find a good balance between\n",
        "#the generator and the discriminator. Therefore, it makes sense to periodically\n",
        "#save the generator model and check how good the generated image looks.\n",
        "def summarize_performance(step, g_model, dataset, n_samples=3):\n",
        "\t# select a sample of input images\n",
        "\t[X_realA, X_realB], _ = generate_real_samples(dataset, n_samples, 1)\n",
        "\t# generate a batch of fake samples\n",
        "\tX_fakeB, _ = generate_fake_samples(g_model, X_realA, 1)\n",
        "\t# scale all pixels from [-1,1] to [0,1]\n",
        "\tX_realA = (X_realA + 1) / 2.0\n",
        "\tX_realB = (X_realB + 1) / 2.0\n",
        "\tX_fakeB = (X_fakeB + 1) / 2.0\n",
        "\t# plot real source images\n",
        "\tfor i in range(n_samples):\n",
        "\t\tplt.subplot(3, n_samples, 1 + i)\n",
        "\t\tplt.axis('off')\n",
        "\t\tplt.imshow(X_realA[i])\n",
        "\t# plot generated target image\n",
        "\tfor i in range(n_samples):\n",
        "\t\tplt.subplot(3, n_samples, 1 + n_samples + i)\n",
        "\t\tplt.axis('off')\n",
        "\t\tplt.imshow(X_fakeB[i])\n",
        "\t# plot real target image\n",
        "\tfor i in range(n_samples):\n",
        "\t\tplt.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
        "\t\tplt.axis('off')\n",
        "\t\tplt.imshow(X_realB[i])\n",
        "\t# save plot to file\n",
        "\tfilename1 = 'plot_%06d.png' % (step+1)\n",
        "\tplt.savefig(filename1)\n",
        "\tplt.close()\n",
        "\t# save the generator model\n",
        "\tfilename2 = 'model_%06d.h5' % (step+1)\n",
        "\tg_model.save(filename2)\n",
        "\tprint('>Saved: %s and %s' % (filename1, filename2))"
      ],
      "metadata": {
        "id": "cVgyNiSZHw0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train pix2pix models\n",
        "def train(d_model, g_model, gan_model, dataset, n_epochs=100, n_batch=1):\n",
        "\t# determine the output square shape of the discriminator\n",
        "\tn_patch = d_model.output_shape[1]\n",
        "\t# unpack dataset\n",
        "\ttrainA, trainB = dataset\n",
        "\t# calculate the number of batches per training epoch\n",
        "\tbat_per_epo = int(len(trainA) / n_batch)\n",
        "\t# calculate the number of training iterations\n",
        "\tn_steps = bat_per_epo * n_epochs\n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(n_steps):\n",
        "\t\t# select a batch of real samples\n",
        "\t\t[X_realA, X_realB], y_real = generate_real_samples(dataset, n_batch, n_patch)\n",
        "\t\t# generate a batch of fake samples\n",
        "\t\tX_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_patch)\n",
        "\t\t# update discriminator for real samples\n",
        "\t\td_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n",
        "\t\t# update discriminator for generated samples\n",
        "\t\td_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n",
        "\t\t# update the generator\n",
        "\t\tg_loss, _, _ = gan_model.train_on_batch(X_realA, [y_real, X_realB])\n",
        "\t\t# summarize performance\n",
        "\t\tprint('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, d_loss1, d_loss2, g_loss))\n",
        "\t\t# summarize model performance\n",
        "\t\tif (i+1) % (bat_per_epo * 10) == 0:\n",
        "\t\t\tsummarize_performance(i, g_model, dataset)"
      ],
      "metadata": {
        "id": "v9UV8AuiH5dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FlSfSHN8H99h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "clmnN5FHH-6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/maps.tar.gz"
      ],
      "metadata": {
        "id": "WbinEspDH_qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzvf /content/maps.tar.gz"
      ],
      "metadata": {
        "id": "qV465c07JPeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "from numpy import asarray, load\n",
        "from numpy import vstack\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import load_img\n",
        "from numpy import savez_compressed\n",
        "from matplotlib import pyplot\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "UXprRywiJwuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load all images in a directory into memory\n",
        "def load_images(path, size=(256,512)):\n",
        "\tsrc_list, tar_list = list(), list()\n",
        "\t# enumerate filenames in directory, assume all are images\n",
        "\tfor filename in listdir(path):\n",
        "\t\t# load and resize the image\n",
        "\t\tpixels = load_img(path + filename, target_size=size)\n",
        "\t\t# convert to numpy array\n",
        "\t\tpixels = img_to_array(pixels)\n",
        "\t\t# split into satellite and map\n",
        "\t\tsat_img, map_img = pixels[:, :256], pixels[:, 256:]\n",
        "\t\tsrc_list.append(sat_img)\n",
        "\t\ttar_list.append(map_img)\n",
        "\treturn [asarray(src_list), asarray(tar_list)]"
      ],
      "metadata": {
        "id": "vvsbwUkTJ8xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset path\n",
        "path = '/content/maps/train/'\n",
        "# load dataset\n",
        "[src_images, tar_images] = load_images(path)\n",
        "print('Loaded: ', src_images.shape, tar_images.shape)"
      ],
      "metadata": {
        "id": "ITYyxqdUKA_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 3\n",
        "for i in range(n_samples):\n",
        "\tpyplot.subplot(2, n_samples, 1 + i)\n",
        "\tpyplot.axis('off')\n",
        "\tpyplot.imshow(src_images[i].astype('uint8'))\n",
        "# plot target image\n",
        "for i in range(n_samples):\n",
        "\tpyplot.subplot(2, n_samples, 1 + n_samples + i)\n",
        "\tpyplot.axis('off')\n",
        "\tpyplot.imshow(tar_images[i].astype('uint8'))\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "opI9lyUfKIao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_shape = src_images.shape[1:]\n",
        "# define the models\n",
        "d_model = define_discriminator(image_shape)\n",
        "g_model = define_generator(image_shape)\n",
        "# define the composite model\n",
        "gan_model = define_gan(g_model, d_model, image_shape)\n",
        "\n",
        "#Define data\n",
        "# load and prepare training images\n",
        "data = [src_images, tar_images]"
      ],
      "metadata": {
        "id": "EYCKo3WyKPPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        "\t# load compressed arrays\n",
        "\t# unpack arrays\n",
        "\tX1, X2 = data[0], data[1]\n",
        "\t# scale from [0,255] to [-1,1]\n",
        "\tX1 = (X1 - 127.5) / 127.5\n",
        "\tX2 = (X2 - 127.5) / 127.5\n",
        "\treturn [X1, X2]\n",
        "\n",
        "dataset = preprocess_data(data)\n",
        "\n",
        "from datetime import datetime\n",
        "start1 = datetime.now()\n",
        "\n",
        "train(d_model, g_model, gan_model, dataset, n_epochs=10, n_batch=1)\n",
        "#Reports parameters for each batch (total 1096) for each epoch.\n",
        "#For 10 epochs we should see 10960\n",
        "\n",
        "stop1 = datetime.now()\n",
        "#Execution time of the model\n",
        "execution_time = stop1-start1\n",
        "print(\"Execution time is: \", execution_time)"
      ],
      "metadata": {
        "id": "P4jOusCFKYyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reports parameters for each batch (total 1096) for each epoch.\n",
        "#For 10 epochs we should see 10960\n",
        "\n",
        "#################################################\n",
        "\n",
        "#Test trained model on a few images...\n",
        "\n",
        "from keras.models import load_model\n",
        "from numpy.random import randint\n",
        "model = load_model('saved_model_10epochs.h5')\n",
        "\n",
        "# plot source, generated and target images\n",
        "def plot_images(src_img, gen_img, tar_img):\n",
        "\timages = vstack((src_img, gen_img, tar_img))\n",
        "\t# scale from [-1,1] to [0,1]\n",
        "\timages = (images + 1) / 2.0\n",
        "\ttitles = ['Source', 'Generated', 'Expected']\n",
        "\t# plot images row by row\n",
        "\tfor i in range(len(images)):\n",
        "\t\t# define subplot\n",
        "\t\tpyplot.subplot(1, 3, 1 + i)\n",
        "\t\t# turn off axis\n",
        "\t\tpyplot.axis('off')\n",
        "\t\t# plot raw pixel data\n",
        "\t\tpyplot.imshow(images[i])\n",
        "\t\t# show title\n",
        "\t\tpyplot.title(titles[i])\n",
        "\tpyplot.show()\n",
        "\n",
        "\n",
        "\n",
        "[X1, X2] = dataset\n",
        "# select random example\n",
        "ix = randint(0, len(X1), 1)\n",
        "src_image, tar_image = X1[ix], X2[ix]\n",
        "# generate image from source\n",
        "gen_image = model.predict(src_image)\n",
        "# plot all three images\n",
        "plot_images(src_image, gen_image, tar_image)"
      ],
      "metadata": {
        "id": "nITV-IC4Kh9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('saved_model_10epochs.h5')"
      ],
      "metadata": {
        "id": "lJHCJLBaKvqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_VF4r2byK78e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}